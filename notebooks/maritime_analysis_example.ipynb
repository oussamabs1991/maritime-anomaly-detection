{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245309e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Maritime Vessel Classification - Exploratory Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrates how to use the Maritime Anomaly Detection pipeline for exploratory data analysis and model experimentation.\\n\",\n",
    "    \"\\n\",\n",
    "    \"## Setup\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import sys\\n\",\n",
    "    \"from pathlib import Path\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Add src to path\\n\",\n",
    "    \"sys.path.append(str(Path().absolute().parent / \\\"src\\\"))\\n\",\n",
    "    \"\\n\",\n",
    "    \"import numpy as np\\n\",\n",
    "    \"import pandas as pd\\n\",\n",
    "    \"import matplotlib.pyplot as plt\\n\",\n",
    "    \"import seaborn as sns\\n\",\n",
    "    \"from loguru import logger\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Import pipeline components\\n\",\n",
    "    \"from src.config import config\\n\",\n",
    "    \"from src.pipeline import MaritimePipeline\\n\",\n",
    "    \"from src.data import AISDataLoader, AISPreprocessor\\n\",\n",
    "    \"from src.features import VesselFeatureExtractor, TrajectoryKalmanFilter\\n\",\n",
    "    \"from src.utils import ModelEvaluator, ModelVisualizer\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Configure plotting\\n\",\n",
    "    \"plt.style.use('default')\\n\",\n",
    "    \"sns.set_palette(\\\"husl\\\")\\n\",\n",
    "    \"%matplotlib inline\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Data Loading and Exploration\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's start by loading and exploring the AIS data.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Update these paths for your data\\n\",\n",
    "    \"ZIP_FILE_PATH = \\\"../data/AIS_2024_10_24.zip\\\"\\n\",\n",
    "    \"CSV_FILE_NAME = \\\"AIS_2024_10_24.csv\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Create data loader\\n\",\n",
    "    \"data_loader = AISDataLoader()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# For this example, we'll work with sample data\\n\",\n",
    "    \"config.TEST_MODE = True\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Extract and load data\\n\",\n",
    "    \"if not Path(\\\"../data/processed/sample_ais_data.csv\\\").exists():\\n\",\n",
    "    \"    csv_path = data_loader.extract_zip_file(ZIP_FILE_PATH, \\\"../data/raw/extracted\\\")\\n\",\n",
    "    \"    sample_path = data_loader.create_sample_data(csv_path, \\\"../data/processed/sample_ais_data.csv\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    sample_path = \\\"../data/processed/sample_ais_data.csv\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load the data\\n\",\n",
    "    \"df_raw = data_loader.load_csv_data(sample_path, test_mode=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Loaded {len(df_raw)} records with {len(df_raw.columns)} columns\\\")\\n\",\n",
    "    \"print(f\\\"Unique vessels: {df_raw['MMSI'].nunique()}\\\")\\n\",\n",
    "    \"print(f\\\"Date range: {df_raw['BaseDateTime'].min()} to {df_raw['BaseDateTime'].max()}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Basic data exploration\\n\",\n",
    "    \"df_raw.head()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Data info\\n\",\n",
    "    \"df_raw.info()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Vessel type distribution\\n\",\n",
    "    \"plt.figure(figsize=(12, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.subplot(1, 2, 1)\\n\",\n",
    "    \"vessel_counts = df_raw['VesselType'].value_counts().head(10)\\n\",\n",
    "    \"vessel_counts.plot(kind='bar')\\n\",\n",
    "    \"plt.title('Top 10 Vessel Types (Count)')\\n\",\n",
    "    \"plt.xlabel('Vessel Type')\\n\",\n",
    "    \"plt.ylabel('Count')\\n\",\n",
    "    \"plt.xticks(rotation=45)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.subplot(1, 2, 2)\\n\",\n",
    "    \"vessel_props = df_raw['VesselType'].value_counts(normalize=True).head(10)\\n\",\n",
    "    \"vessel_props.plot(kind='pie', autopct='%1.1f%%')\\n\",\n",
    "    \"plt.title('Top 10 Vessel Types (Proportion)')\\n\",\n",
    "    \"plt.ylabel('')\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Data Preprocessing\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's preprocess the data and see how it affects the dataset.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Apply preprocessing\\n\",\n",
    "    \"preprocessor = AISPreprocessor()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# First, basic validation\\n\",\n",
    "    \"df_validated = data_loader.validate_required_columns(df_raw)\\n\",\n",
    "    \"df_validated = data_loader.basic_data_validation(df_validated)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"After validation: {len(df_validated)} records\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Apply full preprocessing\\n\",\n",
    "    \"df_processed = preprocessor.preprocess_pipeline(df_validated)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"After preprocessing: {len(df_processed)} records\\\")\\n\",\n",
    "    \"print(f\\\"Unique vessels: {df_processed['MMSI'].nunique()}\\\")\\n\",\n",
    "    \"print(f\\\"Unique trajectories: {df_processed['traj_id'].nunique()}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Compare vessel type distributions before and after preprocessing\\n\",\n",
    "    \"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Before\\n\",\n",
    "    \"df_raw['VesselType'].value_counts().plot(kind='bar', ax=ax1)\\n\",\n",
    "    \"ax1.set_title('Vessel Types - Before Preprocessing')\\n\",\n",
    "    \"ax1.set_xlabel('Vessel Type')\\n\",\n",
    "    \"ax1.set_ylabel('Count')\\n\",\n",
    "    \"ax1.tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# After\\n\",\n",
    "    \"df_processed['VesselType'].value_counts().plot(kind='bar', ax=ax2)\\n\",\n",
    "    \"ax2.set_title('Vessel Types - After Preprocessing')\\n\",\n",
    "    \"ax2.set_xlabel('Vessel Type')\\n\",\n",
    "    \"ax2.set_ylabel('Count')\\n\",\n",
    "    \"ax2.tick_params(axis='x', rotation=45)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Trajectory Analysis\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's examine vessel trajectories and apply Kalman filtering.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Trajectory statistics\\n\",\n",
    "    \"traj_stats = df_processed.groupby(['MMSI', 'traj_id']).size()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Trajectory length statistics:\\\")\\n\",\n",
    "    \"print(traj_stats.describe())\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(10, 6))\\n\",\n",
    "    \"plt.hist(traj_stats, bins=30, alpha=0.7, edgecolor='black')\\n\",\n",
    "    \"plt.xlabel('Trajectory Length (number of points)')\\n\",\n",
    "    \"plt.ylabel('Frequency')\\n\",\n",
    "    \"plt.title('Distribution of Trajectory Lengths')\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Apply Kalman filtering\\n\",\n",
    "    \"kalman_filter = TrajectoryKalmanFilter()\\n\",\n",
    "    \"df_smoothed = kalman_filter.process_all_trajectories(df_processed)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Validate smoothing quality\\n\",\n",
    "    \"quality_metrics = kalman_filter.validate_smoothing_quality(df_smoothed)\\n\",\n",
    "    \"print(\\\"Kalman filter quality metrics:\\\")\\n\",\n",
    "    \"for metric, value in quality_metrics.items():\\n\",\n",
    "    \"    print(f\\\"  {metric}: {value}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Visualize smoothing effect\\n\",\n",
    "    \"# Pick a random trajectory to visualize\\n\",\n",
    "    \"sample_vessel = df_smoothed['MMSI'].iloc[0]\\n\",\n",
    "    \"sample_traj = df_smoothed['traj_id'].iloc[0]\\n\",\n",
    "    \"\\n\",\n",
    "    \"sample_data = df_smoothed[\\n\",\n",
    "    \"    (df_smoothed['MMSI'] == sample_vessel) & \\n\",\n",
    "    \"    (df_smoothed['traj_id'] == sample_traj)\\n\",\n",
    "    \"].copy().sort_values('BaseDateTime')\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(sample_data) > 5:\\n\",\n",
    "    \"    plt.figure(figsize=(12, 8))\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot original vs smoothed coordinates\\n\",\n",
    "    \"    plt.subplot(2, 2, 1)\\n\",\n",
    "    \"    plt.plot(sample_data['LON'], sample_data['LAT'], 'r.-', alpha=0.7, label='Original')\\n\",\n",
    "    \"    plt.plot(sample_data['LON_smoothed'], sample_data['LAT_smoothed'], 'b.-', alpha=0.7, label='Smoothed')\\n\",\n",
    "    \"    plt.xlabel('Longitude')\\n\",\n",
    "    \"    plt.ylabel('Latitude')\\n\",\n",
    "    \"    plt.title('Trajectory: Original vs Smoothed')\\n\",\n",
    "    \"    plt.legend()\\n\",\n",
    "    \"    plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot speed over time\\n\",\n",
    "    \"    plt.subplot(2, 2, 2)\\n\",\n",
    "    \"    plt.plot(sample_data['BaseDateTime'], sample_data['SOG'], 'g.-')\\n\",\n",
    "    \"    plt.xlabel('Time')\\n\",\n",
    "    \"    plt.ylabel('Speed over Ground (knots)')\\n\",\n",
    "    \"    plt.title('Speed Profile')\\n\",\n",
    "    \"    plt.xticks(rotation=45)\\n\",\n",
    "    \"    plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot course over time\\n\",\n",
    "    \"    plt.subplot(2, 2, 3)\\n\",\n",
    "    \"    plt.plot(sample_data['BaseDateTime'], sample_data['COG'], 'm.-')\\n\",\n",
    "    \"    plt.xlabel('Time')\\n\",\n",
    "    \"    plt.ylabel('Course over Ground (degrees)')\\n\",\n",
    "    \"    plt.title('Course Profile')\\n\",\n",
    "    \"    plt.xticks(rotation=45)\\n\",\n",
    "    \"    plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    # Plot smoothing differences\\n\",\n",
    "    \"    plt.subplot(2, 2, 4)\\n\",\n",
    "    \"    lat_diff = np.abs(sample_data['LAT'] - sample_data['LAT_smoothed'])\\n\",\n",
    "    \"    lon_diff = np.abs(sample_data['LON'] - sample_data['LON_smoothed'])\\n\",\n",
    "    \"    plt.plot(sample_data['BaseDateTime'], lat_diff, 'r.-', label='LAT diff')\\n\",\n",
    "    \"    plt.plot(sample_data['BaseDateTime'], lon_diff, 'b.-', label='LON diff')\\n\",\n",
    "    \"    plt.xlabel('Time')\\n\",\n",
    "    \"    plt.ylabel('Absolute Difference')\\n\",\n",
    "    \"    plt.title('Smoothing Effect')\\n\",\n",
    "    \"    plt.xticks(rotation=45)\\n\",\n",
    "    \"    plt.legend()\\n\",\n",
    "    \"    plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"Sample trajectory too short for visualization\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Feature Extraction\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's extract features from the processed trajectories.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Extract features\\n\",\n",
    "    \"feature_extractor = VesselFeatureExtractor()\\n\",\n",
    "    \"features_df, X, y = feature_extractor.create_feature_pipeline(df_smoothed)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Features extracted:\\\")\\n\",\n",
    "    \"print(f\\\"  Samples: {X.shape[0]}\\\")\\n\",\n",
    "    \"print(f\\\"  Features: {X.shape[1]}\\\")\\n\",\n",
    "    \"print(f\\\"  Classes: {len(np.unique(y))}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"\\\\nFeature columns:\\\")\\n\",\n",
    "    \"for i, col in enumerate(X.columns):\\n\",\n",
    "    \"    print(f\\\"  {i+1:2d}. {col}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feature distribution analysis\\n\",\n",
    "    \"# Select a few interesting features for visualization\\n\",\n",
    "    \"interesting_features = [\\n\",\n",
    "    \"    'Length', 'Width', 'Speed_mean', 'Speed_max', \\n\",\n",
    "    \"    'Total_distance_km', 'Duration_hr', 'Stop_ratio', 'High_speed_ratio'\\n\",\n",
    "    \"]\\n\",\n",
    "    \"\\n\",\n",
    "    \"available_features = [f for f in interesting_features if f in X.columns]\\n\",\n",
    "    \"\\n\",\n",
    "    \"if len(available_features) >= 4:\\n\",\n",
    "    \"    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\\n\",\n",
    "    \"    axes = axes.ravel()\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    for i, feature in enumerate(available_features[:4]):\\n\",\n",
    "    \"        X[feature].hist(bins=30, alpha=0.7, ax=axes[i])\\n\",\n",
    "    \"        axes[i].set_title(f'Distribution of {feature}')\\n\",\n",
    "    \"        axes[i].set_xlabel(feature)\\n\",\n",
    "    \"        axes[i].set_ylabel('Frequency')\\n\",\n",
    "    \"        axes[i].grid(True, alpha=0.3)\\n\",\n",
    "    \"    \\n\",\n",
    "    \"    plt.tight_layout()\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(f\\\"Available features for plotting: {available_features}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feature correlation analysis\\n\",\n",
    "    \"correlation_matrix = X.corr()\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(12, 10))\\n\",\n",
    "    \"mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\\n\",\n",
    "    \"sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', center=0)\\n\",\n",
    "    \"plt.title('Feature Correlation Matrix')\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Show highly correlated features\\n\",\n",
    "    \"high_corr = correlation_matrix.abs() > 0.8\\n\",\n",
    "    \"high_corr_pairs = []\\n\",\n",
    "    \"for i in range(len(correlation_matrix.columns)):\\n\",\n",
    "    \"    for j in range(i+1, len(correlation_matrix.columns)):\\n\",\n",
    "    \"        if high_corr.iloc[i, j]:\\n\",\n",
    "    \"            high_corr_pairs.append((\\n\",\n",
    "    \"                correlation_matrix.columns[i],\\n\",\n",
    "    \"                correlation_matrix.columns[j],\\n\",\n",
    "    \"                correlation_matrix.iloc[i, j]\\n\",\n",
    "    \"            ))\\n\",\n",
    "    \"\\n\",\n",
    "    \"if high_corr_pairs:\\n\",\n",
    "    \"    print(\\\"Highly correlated feature pairs (|correlation| > 0.8):\\\")\\n\",\n",
    "    \"    for feat1, feat2, corr in high_corr_pairs:\\n\",\n",
    "    \"        print(f\\\"  {feat1} <-> {feat2}: {corr:.3f}\\\")\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"No highly correlated feature pairs found\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Model Training and Evaluation\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's train the complete ensemble model and evaluate its performance.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create and run the complete pipeline\\n\",\n",
    "    \"pipeline = MaritimePipeline()\\n\",\n",
    "    \"\\n\",\n",
    "    \"# For demonstration, we'll use the already processed data\\n\",\n",
    "    \"# In practice, you would run: pipeline.run_complete_pipeline(ZIP_FILE_PATH, CSV_FILE_NAME)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Prepare sequence data\\n\",\n",
    "    \"sequences = pipeline.prepare_sequence_data(df_smoothed, features_df)\\n\",\n",
    "    \"print(f\\\"Sequence data shape: {sequences.shape}\\\")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Split and scale data\\n\",\n",
    "    \"(X_train, X_test, y_train, y_test, \\n\",\n",
    "    \" seq_train, seq_test, class_weights) = pipeline.split_and_scale_data(X.values, y.values, sequences)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Training set: {X_train.shape[0]} samples\\\")\\n\",\n",
    "    \"print(f\\\"Test set: {X_test.shape[0]} samples\\\")\\n\",\n",
    "    \"print(f\\\"Class weights: {class_weights}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Train the ensemble\\n\",\n",
    "    \"logger.info(\\\"Training ensemble model...\\\")\\n\",\n",
    "    \"ensemble = pipeline.train_ensemble(X_train, y_train, seq_train)\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Ensemble training completed!\\\")\\n\",\n",
    "    \"print(f\\\"Base models: {list(ensemble.base_models.keys())}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Make predictions\\n\",\n",
    "    \"predictions = ensemble.predict(X_test, seq_test)\\n\",\n",
    "    \"probabilities = ensemble.predict_proba(X_test, seq_test)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Convert labels back if needed\\n\",\n",
    "    \"if hasattr(pipeline.label_encoder, 'classes_'):\\n\",\n",
    "    \"    y_test_orig = pipeline.label_encoder.inverse_transform(y_test)\\n\",\n",
    "    \"    class_names = list(pipeline.label_encoder.classes_)\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    y_test_orig = y_test\\n\",\n",
    "    \"    class_names = None\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Predictions shape: {predictions.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Probabilities shape: {probabilities.shape}\\\")\\n\",\n",
    "    \"print(f\\\"Class names: {class_names}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Evaluate model performance\\n\",\n",
    "    \"evaluator = ModelEvaluator()\\n\",\n",
    "    \"results = evaluator.evaluate_model(\\n\",\n",
    "    \"    y_test_orig, predictions, probabilities, class_names, \\\"Ensemble\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Print detailed report\\n\",\n",
    "    \"evaluator.print_detailed_report(\\\"Ensemble\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Visualizations\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's create comprehensive visualizations of the model performance.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Create visualizer\\n\",\n",
    "    \"visualizer = ModelVisualizer(save_plots=False)  # Don't save plots in notebook\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Confusion matrix\\n\",\n",
    "    \"visualizer.plot_confusion_matrix(y_test_orig, predictions, class_names)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Classification report\\n\",\n",
    "    \"visualizer.plot_classification_report(y_test_orig, predictions, class_names)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# ROC curves\\n\",\n",
    "    \"visualizer.plot_roc_curves(y_test_orig, probabilities, class_names)\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Feature importance from ensemble\\n\",\n",
    "    \"importance_df = ensemble.get_base_model_importance()\\n\",\n",
    "    \"if not importance_df.empty:\\n\",\n",
    "    \"    visualizer.plot_feature_importance(importance_df, \\\"Base Model Importance\\\")\\n\",\n",
    "    \"    plt.show()\\n\",\n",
    "    \"else:\\n\",\n",
    "    \"    print(\\\"No feature importance data available\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Complete evaluation dashboard\\n\",\n",
    "    \"visualizer.create_evaluation_dashboard(\\n\",\n",
    "    \"    y_test_orig, predictions, probabilities, class_names, \\\"Maritime Ensemble\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"plt.show()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Analysis and Insights\\n\",\n",
    "    \"\\n\",\n",
    "    \"Let's analyze the results and extract insights.\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Performance by vessel type\\n\",\n",
    "    \"from sklearn.metrics import classification_report\\n\",\n",
    "    \"report = classification_report(y_test_orig, predictions, output_dict=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"per_class_df = pd.DataFrame(report).T\\n\",\n",
    "    \"per_class_df = per_class_df[per_class_df.index.isin(class_names)]\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(\\\"Performance by Vessel Type:\\\")\\n\",\n",
    "    \"print(per_class_df[['precision', 'recall', 'f1-score', 'support']].round(3))\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Prediction confidence analysis\\n\",\n",
    "    \"max_probs = np.max(probabilities, axis=1)\\n\",\n",
    "    \"correct_predictions = (predictions == y_test_orig)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.figure(figsize=(12, 5))\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.subplot(1, 2, 1)\\n\",\n",
    "    \"plt.hist([max_probs[correct_predictions], max_probs[~correct_predictions]], \\n\",\n",
    "    \"         bins=20, alpha=0.7, label=['Correct', 'Incorrect'], density=True)\\n\",\n",
    "    \"plt.xlabel('Maximum Prediction Probability')\\n\",\n",
    "    \"plt.ylabel('Density')\\n\",\n",
    "    \"plt.title('Prediction Confidence Distribution')\\n\",\n",
    "    \"plt.legend()\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.subplot(1, 2, 2)\\n\",\n",
    "    \"confidence_bins = np.arange(0, 1.1, 0.1)\\n\",\n",
    "    \"binned_confidence = np.digitize(max_probs, confidence_bins)\\n\",\n",
    "    \"accuracy_by_confidence = []\\n\",\n",
    "    \"for i in range(1, len(confidence_bins)):\\n\",\n",
    "    \"    mask = binned_confidence == i\\n\",\n",
    "    \"    if mask.sum() > 0:\\n\",\n",
    "    \"        accuracy = correct_predictions[mask].mean()\\n\",\n",
    "    \"        accuracy_by_confidence.append(accuracy)\\n\",\n",
    "    \"    else:\\n\",\n",
    "    \"        accuracy_by_confidence.append(0)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.plot(confidence_bins[1:], accuracy_by_confidence, 'bo-')\\n\",\n",
    "    \"plt.xlabel('Prediction Confidence (binned)')\\n\",\n",
    "    \"plt.ylabel('Accuracy')\\n\",\n",
    "    \"plt.title('Accuracy vs Prediction Confidence')\\n\",\n",
    "    \"plt.grid(True, alpha=0.3)\\n\",\n",
    "    \"\\n\",\n",
    "    \"plt.tight_layout()\\n\",\n",
    "    \"plt.show()\\n\",\n",
    "    \"\\n\",\n",
    "    \"print(f\\\"Average confidence for correct predictions: {max_probs[correct_predictions].mean():.3f}\\\")\\n\",\n",
    "    \"print(f\\\"Average confidence for incorrect predictions: {max_probs[~correct_predictions].mean():.3f}\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## Conclusion\\n\",\n",
    "    \"\\n\",\n",
    "    \"This notebook demonstrated the complete maritime vessel classification pipeline, from data loading to model evaluation. Key takeaways:\\n\",\n",
    "    \"\\n\",\n",
    "    \"1. **Data Quality**: Preprocessing significantly reduces the dataset size but improves data quality\\n\",\n",
    "    \"2. **Feature Engineering**: The extracted features capture both static vessel characteristics and dynamic behavioral patterns\\n\",\n",
    "    \"3. **Model Performance**: The ensemble approach combines multiple model types for robust classification\\n\",\n",
    "    \"4. **Evaluation**: Comprehensive metrics and visualizations provide insights into model performance\\n\",\n",
    "    \"\\n\",\n",
    "    \"### Next Steps\\n\",\n",
    "    \"\\n\",\n",
    "    \"- Experiment with different feature engineering approaches\\n\",\n",
    "    \"- Try different ensemble configurations\\n\",\n",
    "    \"- Analyze prediction errors to identify improvement opportunities\\n\",\n",
    "    \"- Test on different geographic regions or time periods\\n\",\n",
    "    \"- Implement online learning for adapting to new data patterns\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save the trained model for later use\\n\",\n",
    "    \"model_path = pipeline.save_model(\\\"../data/models/notebook_trained_model.joblib\\\")\\n\",\n",
    "    \"print(f\\\"Model saved to: {model_path}\\\")\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.8.0\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
